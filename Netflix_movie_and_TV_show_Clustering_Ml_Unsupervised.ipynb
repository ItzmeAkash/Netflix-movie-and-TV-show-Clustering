{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItzmeAkash/Netflix-movie-and-TV-show-Clustering/blob/main/Netflix_movie_and_TV_show_Clustering_Ml_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Netflix movie and TV show Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -**  Akash Ps\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ItzmeAkash/Netflix-movie-and-TV-show-Clustering"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Context**\n",
        "\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "\n",
        "In this Project, You are required to do\n",
        "\n",
        "* Exploratory Data Analysis\n",
        "* Understanding What type content is available in different countries\n",
        "* If Netflix has been increasingly focuing on TV rather then Movie in recent years\n",
        "* Clustering similar content by matching text-based features"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p8kQNTkKtku7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data_path = \"/content/drive/MyDrive/Self Projects/AlmaBetter Capstone Projects/Ml Unsupervised /NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\"\n",
        "df = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Rows and Column count in the Dataset: Rows= {df.shape[0]}, Columns= {df.shape[1]}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"The total number of duplicated observations in the dataset: {df.duplicated().sum()}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isna().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize = (7,5))\n",
        "sns.heatmap(df.isnull(), cbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plots= sns.barplot(x=df.columns,y=df.isna().sum())\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width() / 2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0, 8),\n",
        "                     textcoords='offset points')"
      ],
      "metadata": {
        "id": "5ss7CWhnuqkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset \"Netflix Movies and TV Shows Clustering\" comprises 12 columns, with only one column having an integer data type. It does not contain any duplicate values, but it does have null values in five columns: **director, cast, country, date_added, and rating**.\n",
        "\n",
        "This dataset provides a valuable resource for exploring trends in the range of movies and TV shows available on Netflix. Additionally, it can be utilized for developing clustering models to categorize similar titles together based on shared attributes such as **genre, country of origin, and rating**."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f\"Available columns:\\n{df.columns.to_list()}\")"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable description of the Netflix Movies and TV Shows Clustering Dataset is as follows:\n",
        "\n",
        "**show_id**: Unique identifier for each movie/show.\n",
        "\n",
        "**type**: Indicates whether the entry is a movie or a TV show.\n",
        "\n",
        "**title**: Name of the movie or TV show.\n",
        "\n",
        "**director**: Name of the director(s) of the movie or TV show.\n",
        "\n",
        "**cast**: Names of the actors and actresses featured in the movie or TV show.\n",
        "\n",
        "**country**: Country or countries where the movie or TV show was produced.\n",
        "\n",
        "**date_added**: Date when the movie or TV show was added to Netflix.\n",
        "\n",
        "**release_year**: Year when the movie or TV show was released.\n",
        "\n",
        "**rating**: TV rating or movie rating of the movie or TV show.\n",
        "\n",
        "**duration**: Length of the movie or TV show in minutes or seasons.\n",
        "\n",
        "**listed_in**: Categories or genres of the movie or TV show.\n",
        "\n",
        "**description**: Brief synopsis or summary of the movie or TV show."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(f\"The number of unique values in:\")\n",
        "for i in df.columns:\n",
        "  print(f\"{i} : {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.Handling Null values from each feature"
      ],
      "metadata": {
        "id": "U0b9sArUvZl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values/Null values Count\n",
        "print(df.isna().sum())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"date_added\"].value_counts()"
      ],
      "metadata": {
        "id": "tbXllk3uvt3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "z-WslQCivzlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['country'].value_counts()"
      ],
      "metadata": {
        "id": "YLEy-7R7v2de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "\n",
        "1. Since 'date_added' and 'rating' has very\n",
        "less percentage of null count so we can drop those observations to avoid any biasness in our clustering model.\n",
        "\n",
        "2. We cannot drop or impute any values in 'director' and 'cast' as the null percentage is comparatevely high and we do not know data of those actual movie/TV shows, so its better to replace those entries with 'unknown'.\n",
        "\n",
        "3. We can fill null values of 'country' with mode as we only have 6% null values and most of the movies/shows are from US only."
      ],
      "metadata": {
        "id": "sLRBJMJ0v4dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing with unknown in null values of director and cast feature\n",
        "df[['director','cast']]=df[['director','cast']].fillna(\"Unknown\")\n",
        "\n",
        "# Imputing null values of country with Mode\n",
        "df['country']=df['country'].fillna(df['country'].mode()[0])\n",
        "\n",
        "# Dropping remaining null values of date_added and rating\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "ZWwV4dWtwCY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking the null values\n",
        "\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "otIeCqbbw0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Handling nested columns i.e 'director', 'cast', 'listed_in' and 'country'"
      ],
      "metadata": {
        "id": "yhNC01Rlw6yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a copy of dataframe and unnest the original one\n",
        "df_new= df.copy()"
      ],
      "metadata": {
        "id": "vbEyDRyvxF3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'Directors' column\n",
        "dir_constraint=df['director'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df1 = pd.DataFrame(dir_constraint, index = df['title'])\n",
        "df1 = df1.stack()\n",
        "df1 = pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "qTi3Co3WxIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Unnesting 'cast' column\n",
        "cast_constraint=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df2 = pd.DataFrame(cast_constraint, index = df['title'])\n",
        "df2 = df2.stack()\n",
        "df2 = pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "COS_rYt_xKT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unnesting 'listed_in' column\n",
        "listed_constraint=df['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(listed_constraint, index = df['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)"
      ],
      "metadata": {
        "id": "o8PNRI9IxMNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Unnesting 'country' column\n",
        "country_constraint=df['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(country_constraint, index = df['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "-mTuBT81xOAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " we have sucessfully separated the nested columns. Now let's just merge all the created dataframe into the single merged dataframe."
      ],
      "metadata": {
        "id": "5AfmxXf5xQ5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging all the unnested dataframes"
      ],
      "metadata": {
        "id": "RkGTsaRDxTYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of (director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# Head of final merged dataframe\n",
        "df7.head()\n"
      ],
      "metadata": {
        "id": "-ojJqR_4xWqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's merge this dataframe with the original one on the left join to avoid information loss."
      ],
      "metadata": {
        "id": "wDiSvtYqxcyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Merging unnested data with the created dataframe in order to make the final dataframe\n",
        "df = df7.merge(df[['type', 'title', 'date_added', 'release_year', 'rating', 'duration','description']],on=['title'],how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Hbk1D88AxRsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.TypeCasting of attributes"
      ],
      "metadata": {
        "id": "HxXn1Y6qxgTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset before typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "GzWH-eNyxjp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Typecasting duration into integer by removing 'min' and 'season' from the end\n",
        "df['duration']= df['duration'].apply(lambda x: int(x.split()[0]))\n",
        "\n",
        "# Typecasting string object to datetime object of date_added column\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "\n",
        "# Extracting date, day, month and year from date_added column\n",
        "df[\"day_added\"]= df[\"date_added\"].dt.day\n",
        "df[\"month_added\"]= df[\"date_added\"].dt.month\n",
        "df[\"year_added\"]= df[\"date_added\"].dt.year\n",
        "\n",
        "# Dropping date_added\n",
        "df.drop('date_added', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "33_scZ42xmZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking info of the dataset after typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "DAvkusFzyKDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Binning of Rating attribute"
      ],
      "metadata": {
        "id": "QV_gafizyTG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **create bins as following:**\n",
        "\n",
        "Adult Content: TV-MA, NC-17, R\n",
        "\n",
        "Children Content: TV-PG, PG, TV-G, G\n",
        "\n",
        "Teen Content: PG-13, TV-14\n",
        "\n",
        "Family-friendly Content: TV-Y, TV-Y7, TV-Y7-FV\n",
        "\n",
        "Not Rated: NR, UR"
      ],
      "metadata": {
        "id": "fPjIN46tyXBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning the values in the rating column\n",
        "rating_map = {'TV-MA':'Adult Content',\n",
        "              'R':'Adult Content',\n",
        "              'PG-13':'Teen Content',\n",
        "              'TV-14':'Teen Content',\n",
        "              'TV-PG':'Children Content',\n",
        "              'NR':'Not Rated',\n",
        "              'TV-G':'Children Content',\n",
        "              'TV-Y':'Family-friendly Content',\n",
        "              'TV-Y7':'Family-friendly Content',\n",
        "              'PG':'Children Content',\n",
        "              'G':'Children Content',\n",
        "              'NC-17':'Adult Content',\n",
        "              'TV-Y7-FV':'Family-friendly Content',\n",
        "              'UR':'Not Rated'}\n"
      ],
      "metadata": {
        "id": "8uUhtBVfyVw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].replace(rating_map, inplace = True)\n",
        "df['rating'].unique()\n"
      ],
      "metadata": {
        "id": "M__G2biLyg2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking head after binning\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-IJ0FU7GyjqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Separating Movies and TV Shows"
      ],
      "metadata": {
        "id": "d6cM3qHXyqKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearating the dataframes for further analysis\n",
        "df_movies= df[df['type']== 'Movie']\n",
        "df_tvshows= df[df['type']== 'TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(df_movies.shape, df_tvshows.shape)"
      ],
      "metadata": {
        "id": "MRVeBAFeysRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have divided data wrangling into five different sections:\n",
        "\n",
        "1. In this section we have imputed/drop the null values of:\n",
        "* Imputed 'director' and 'cast' with 'Unknown'.\n",
        "* Imputed 'country' with Mode.\n",
        "* Drop null values of 'date_added' and 'rating' (less percentage).\n",
        "\n",
        "2. We have unnested values from following features:\n",
        "\n",
        "* 'director'\n",
        "* 'cast'\n",
        "* 'listed_in'\n",
        "* 'country'\n",
        "We have unnested the values and stored in different dataframes and then merged all the dataframe with the original one using left join in order to get the isolated values of each of the feature.\n",
        "3. * We have typecasted the following features:\n",
        "\n",
        "* 'duration' into integer (Removing min and seasons from the values).\n",
        "* 'date_added' to datetime (Into the required format).\n",
        "\n",
        "* We have also extracted the following features:\n",
        "* 'date' from 'date_added'.\n",
        "* 'month' from 'date_added'.\n",
        "* 'year' from 'date_added'.\n",
        "4. We have seen that the 'rating' column contains various coded categories, so we have decided to create 5 bins and distribute the values accordingly:\n",
        "* Adult: TV-MA, NC-17\n",
        "* Restricted: R, UR\n",
        "* Teen: PG-13, TV-14\n",
        "* All Ages: TV-G, TV-Y, TV-Y7, TV-Y7-FV, PG, G, TV-PG\n",
        "* Not Rated: NR\n",
        "\n",
        "5. Lastly we have splitted the dataframe into two df one is 'df_movies' that contains only Movies and the other is 'df_tvshows' that contains only TV Shows for our further analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 1 (The relative percentage of total number of Movies and TV Shows over Netflix?)"
      ],
      "metadata": {
        "id": "mpP11vJp0QI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "labels = ['TV Show', 'Movie']\n",
        "values = [df.type.value_counts()[1], df.type.value_counts()[0]]\n",
        "\n",
        "# Colors\n",
        "colors = ['#ffd700', '#008000']\n",
        "\n",
        "# Create pie chart\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title_text='Type of Content Watched on Netflix',\n",
        "    title_x=0.5,\n",
        "    height=500,\n",
        "    width=500,\n",
        "    legend=dict(x=0.9),\n",
        "    annotations=[dict(text='Type of Content', font_size=20, showarrow=False)]\n",
        ")\n",
        "\n",
        "# Set colors\n",
        "fig.update_traces(marker=dict(colors=colors))\n"
      ],
      "metadata": {
        "id": "gVjh2l5R0UzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "This graph shows us the percent of TV shows and movie data present on Netflix Data set\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* We can see that the majority of the content on Netflix is movies, which account for around two-thirds of the total content. TV shows make up the remaining one-third of the content.\n",
        "\n",
        "* we can conclude that in the given Data set only 28.3% are TV Shows and 71.7% are Movies."
      ],
      "metadata": {
        "id": "66VUoPUJ0dOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 2 (How content is distributed over Netflix?)"
      ],
      "metadata": {
        "id": "D6pd18Zj0k3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "for i,j,k in ((df, 'Overall',0),(df_movies, 'Movies',1),(df_tvshows, 'TV Shows',2)):\n",
        "  plt.subplot(1,3,k+1)\n",
        "  count= i['rating'].value_counts()\n",
        "  plt.pie(count, labels=count.index,explode=(0,0,0,0,0.5),colors=['orangered','dodgerblue','lightgreen','mediumslateblue','yellow'],\n",
        "          autopct='%1.1f%%', labeldistance=1.1,wedgeprops={\"edgecolor\" : \"black\",'linewidth': 1,'antialiased': True})\n",
        "  plt.title(f\"Distribution of Content Rating on Netflix '{j}'\")\n",
        "  plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9adNZLUN0eON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Why did you pick the specific chart?\n",
        "We chosen this chart to know the percentage of type of content present in the Netflix.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* We found that most of the content present in the Netflix belongs to Adult and the teen categories.\n",
        "\n",
        "* .Another important insight we can see that Family friendly content less in Movies compared to TV Shows"
      ],
      "metadata": {
        "id": "05IeiqC70s48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 3 (Who are the top actors performing in Movies and TV Shows?)"
      ],
      "metadata": {
        "id": "Ita53-0409N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 casts in Movies and TV Shows\n",
        "plt.style.use('default')\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_actor = i.groupby(['Actors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Actors\",x = 'title', data = df_actor, palette='tab10')\n",
        "  plt.title(f'Actors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iCtoYv8e1BOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Why did you pick the specific chart?\n",
        "To know which actors are more popular on Netflix.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* We found an interesting insight that most of the Actors in Movies are from INDIA.\n",
        "\n",
        "* No popular actors from india in TV Shows."
      ],
      "metadata": {
        "id": "TZEY9hDk1iyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 4 (Who are the top Directors directing Movies and TV Shows?)"
      ],
      "metadata": {
        "id": "SBocL3zf1xrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 Directors in Movies and TV Shows\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_director = i.groupby(['Directors']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[1:10]\n",
        "  plots= sns.barplot(y = \"Directors\",x = 'title', data = df_director, palette='Paired')\n",
        "  plt.title(f'Directors appeared in most of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Zd9Yr8O0wwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* To know which director is popular in Movies and which one is popular in TV Shows.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* We found that most of the movies directed by jan suter.\n",
        "\n",
        "* Most TV shows directed by ken burns."
      ],
      "metadata": {
        "id": "KS0_iPRw14Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 5 (What are the top 10 Countries involved in content creation?)"
      ],
      "metadata": {
        "id": "aUpVLI371-oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_country = df.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plt.figure(figsize=(15,6))\n",
        "plots= sns.barplot(y = \"Country\",x = 'title', data = df_country, palette='Set2')\n",
        "plt.xticks(rotation = 60)\n",
        "plt.title('Top 10 Countries for content creation')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P6qT6cN617Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* To know which country produces Maximum number of TV Shows and Movies.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* The United States is the top country producing both movies and TV shows on Netflix. This suggests that Netflix is heavily influenced by American content.\n",
        "\n",
        "* India is the second-highest producer of movies on Netflix, indicating the growing popularity of Bollywood movies worldwide.\n",
        "\n",
        "* Country like canada, france, japan also have significant presence in the data set showing diversity of content on the netflix"
      ],
      "metadata": {
        "id": "TJFiQnxd2yLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 6 (Which Countries has the highest spread of Movies and TV Shows over Netflix)"
      ],
      "metadata": {
        "id": "mAp-yOMu3Em4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing top15 countries with most content\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "sns.countplot(x=df['Country'],order=df['Country'].value_counts().index[0:15],hue=df['type'],palette =\"Set1\")\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 countries with most contents', fontsize=15, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_country = i.groupby(['Country']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Country\",x = 'title', data = df_country, palette='Set1')\n",
        "  plt.title(f'Top 10 countries launching {j} back to back')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cJ2ThMp53L-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "To know which country produces which type of content the most.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* INDIA Produces most amount of Movies in compare to TV Shows.\n",
        "\n",
        "* Japan and South korea produces more TV Shows in compare to Movies."
      ],
      "metadata": {
        "id": "0QhPc1Br3Q4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 7 (Which Genres are Popular in Netflix)"
      ],
      "metadata": {
        "id": "EZcAB-lj3VSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "df_genre = df.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre,palette='Set1')\n",
        "plt.title(f'Most popular genre on Netflix')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plots.bar_label(plots.containers[0])\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_genre = i.groupby(['Genre']).agg({'title':'nunique'}).reset_index().sort_values(by=['title'],ascending=False)[:10]\n",
        "  plots= sns.barplot(y = \"Genre\",x = 'title', data = df_genre, palette='Set1')\n",
        "  plt.title(f'Most popular genre of the {j}')\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  plots.bar_label(plots.containers[0])\n",
        "  plt.yticks(rotation = 45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rg1YLFzd3Y2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* This graph tells us which genre is most popular in Netflix.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* International movies genre is most popular in both the TV Shows and Movies category. Followed by Drama and comedy."
      ],
      "metadata": {
        "id": "UywRz0H83iNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 8 (Total number of Movies/TV Shows released and added per year on Netflix?)"
      ],
      "metadata": {
        "id": "BdVK5LbT32xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_release_year = i.groupby(['release_year']).agg({'title':'nunique'}).reset_index().sort_values(by=['release_year'],ascending=False)[:14]\n",
        "  plots= sns.barplot(x = 'release_year',y= 'title', data = df_release_year, palette='husl')\n",
        "  plt.title(f'{j} released by year')\n",
        "  plt.ylabel(f\"Number of {j} released\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_country = i.groupby(['year_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'year_added',y= 'title', data = df_country, palette='husl')\n",
        "  plt.title(f'{j} added to Netflix by year')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')"
      ],
      "metadata": {
        "id": "Az7tF9_034lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* This graph shows us how many movies and TV Show released and added in a year on Netflix.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* We can see that the number of movies and TV shows added on Netflix has been increasing steadily every year. But since 2018, the number of Movies released on Netflix has been lowered and the number of TV shows released has been significantly increased. In terms of movies and TV Shows addition, in 2020 Number of movies added as compared to 2019 were vey less and on the other side number of TV Shows were more as compare to 2019."
      ],
      "metadata": {
        "id": "3P-AhNq03_mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 9 (Total Number of Movies/TV Shows added per month on Netflix)"
      ],
      "metadata": {
        "id": "1kMM3OEm4FQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_month = i.groupby(['month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['month_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'month_added',y='title', data = df_month, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by month')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='center',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bbq0AmU64IPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* We have plotted this graph to know in which month the movie/tv shows added is maximum and in which year minimum.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* We found that October, November and December are the most popular months for TV shows addition.\n",
        "\n",
        "* January, October and December are the most popular months for movie addition.\n",
        "\n",
        "* February is the least popular month for the movies and TV shows to be added on Netflix"
      ],
      "metadata": {
        "id": "RRXonEDo4ND_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 10 (Total Number of Movies/TV Shows added per day on Netflix)"
      ],
      "metadata": {
        "id": "K7KQr-364aXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "for i,j,k in ((df_movies, 'Movies',0),(df_tvshows, 'TV Shows',1)):\n",
        "  plt.subplot(1,2,k+1)\n",
        "  df_day = i.groupby(['day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['day_added'],ascending=False)\n",
        "  plots= sns.barplot(x = 'day_added',y='title', data = df_day, palette='husl')\n",
        "  plt.title(f'{j} added added to Netflix by day')\n",
        "  plt.ylabel(f\"Number of {j} added on Netflix\")\n",
        "  plt.grid(linestyle='--', linewidth=0.3)\n",
        "  for bar in plots.patches:\n",
        "     plots.annotate(bar.get_height(),\n",
        "                    (bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height()), ha='center', va='bottom',\n",
        "                    size=12, xytext=(0, 8),\n",
        "                    textcoords='offset points', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3lqKDay4b69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* This graph shows us the day when most of the movies added in a month.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* From the above bar plots, it can be observed that most of the movies and TV shows are added at the beginning or middle of the month. It could be because most people tend to have more free time at the beginning of the month after getting paid, and releasing new content during that time could increase viewership. By releasing new content at the beginning and middle of the month, subscribers are more likely to feel that they are getting value for their money, which could lead to increased retention rates."
      ],
      "metadata": {
        "id": "EbnrIFKv4fAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 11 (What is the Month-wise number of content added in each year on Netflix?)"
      ],
      "metadata": {
        "id": "fqVL-VWZ4jSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','month_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x = 'year_added',y='title', data = df_year_month, palette = 'hls', hue=df_year_month['month_added'], marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ZWvCPRr4m8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "This bivariate graph helps us in knowing which month is dominating in adding movie/tvshows in a year.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* We can see that there is no specific trend is followed, Instead of this some consecutive years shows month wise trend.\n",
        "\n",
        "* From 2008 to 2009 we see movies added in the month of February, and from 2009 to 2011 movies added in the month of February and October.\n",
        "\n",
        "* After 2015 majority content added in the month of october to december"
      ],
      "metadata": {
        "id": "dHjSd1_P4pRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 12 (What is the Day-wise number of content added in each year on Netflix?)"
      ],
      "metadata": {
        "id": "NZgG6qX74vqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "df_year_month = df.groupby(['year_added','day_added']).agg({'title':'nunique'}).reset_index().sort_values(by=['year_added'],ascending=False)\n",
        "sns.lineplot(x = 'year_added',y='title', data = df_year_month, palette = 'hls', hue=df_year_month['day_added'], marker='o')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bU4VKvjf4xX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* This graph help us in knowing which day is more frequent in movie addition year wise.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "* Movies from 2008 to 2009 added on 5th day of the month.\n",
        "* Movies from 2009 to 2010 added on 15th day of the month.\n",
        "* Most of the movies from 2010 to 2012 added in the month end.\n",
        "* From 2015 onwords most of the movies are added in month end or mid month."
      ],
      "metadata": {
        "id": "IpjiW1AI41eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 13 (What is the Distribution of Duration of contents over Netflix?)"
      ],
      "metadata": {
        "id": "_CGLdlq-48a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the distribution of Movie Durations\n",
        "plt.figure(figsize=(10,7))\n",
        "plots= sns.distplot(df_movies['duration'],kde=False, color=['skyblue'])\n",
        "plt.title('Distplot with Normal distribution for Movies',fontweight=\"bold\")\n",
        "for bar in plots.patches:\n",
        "   plots.annotate(bar.get_height(),\n",
        "                  (bar.get_x() + bar.get_width() / 2,\n",
        "                   bar.get_height()), ha='center', va='bottom',\n",
        "                  size=7, xytext=(0, 5),\n",
        "                  textcoords='offset points', rotation=90)"
      ],
      "metadata": {
        "id": "xaeOqUa247RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "df_duration = df_tvshows.groupby(['duration']).agg({'title':'nunique'}).reset_index().sort_values(by=['duration'],ascending=False)\n",
        "plots= sns.barplot(x = 'duration',y='title', data = df_duration, palette='husl')\n",
        "plt.title(f'Barplot of TV Shows Duration')\n",
        "plt.ylabel(f\"Content count\")\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "for bar in plots.patches:\n",
        "   plots.annotate(bar.get_height(),\n",
        "                  (bar.get_x() + bar.get_width() / 2,\n",
        "                   bar.get_height()), ha='center', va='bottom',\n",
        "                  size=12, xytext=(0, 8),\n",
        "                  textcoords='offset points', rotation=90)"
      ],
      "metadata": {
        "id": "FdxbwAT15E3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "* To know the duration distribution for Movies and TV Shows on Netflix.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "* The histogram of the distribution of movie durations in minutes on Netflix shows that the **majority of movies on Netflix have a duration between 80 to 120 minutes. **\n",
        "\n",
        "* The countplot of the distribution of TV show durations in seasons on Netflix shows that the most common duration for TV shows on Netflix is one season, followed by two seasons."
      ],
      "metadata": {
        "id": "mPSvZ6st5IUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1:\n",
        "\n",
        "* **Null Hypothesis**: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "* **Alternative Hypothesis**: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Hypothetical Statement 2:\n",
        "\n",
        "* **Null Hypothesis**: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "* **Alternative Hypothesis**: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Hypothetical Statement 3:\n",
        "\n",
        "* **Null Hypothesis**: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "* **Alternative Hypothesis**: The proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "**Alternative Hypothesis**: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Subset the data to only include drama and comedy movies\n",
        "subset = df[df['Genre'].str.contains('Dramas') | df['Genre'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['Genre'].str.contains('Dramas')]) / len(subset)\n",
        "comedy_prop = len(subset[subset['Genre'].str.contains('Comedies')]) / len(subset)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "We conclude that there is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "2l3W0l9e51O-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The statistical test we have used to obtain the P-value is the z-test for proportions"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions was chosen because we are comparing the proportions of two categorical variables (drama movies and comedy movies) in a sample. The null hypothesis and alternative hypothesis are about the difference in proportions, and we want to determine if the observed difference in proportions is statistically significant or not. The z-test for proportions is appropriate for this situation because it allows us to compare two proportions and calculate the probability of observing the difference we see in our sample if the null hypothesis were true."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "**Alternative Hypothesis**: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# To test this hypothesis, we perform a two-sample t-test.\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create separate dataframes for TV shows in 2020 and 2021\n",
        "tv_2020 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2020)]\n",
        "tv_2021 = df[(df['type'] == 'TV Show') & (df['release_year'] == 2021)]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t, p = ttest_ind(tv_2020['duration'].astype(int),\n",
        "                 tv_2021['duration'].astype(int), equal_var=False)\n",
        "print('t-value: ', t)\n",
        "print('p-value: ', p)\n",
        "\n",
        "# Print the results\n",
        "if p < 0.05:\n",
        "    print('Reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.')\n",
        "else:\n",
        "    print('Failed to reject null hypothesis. \\nThe average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.')\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain the P-Value is a two-sample t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of two different samples (TV shows added in 2020 vs TV shows added in 2021) to determine whether they are significantly different. Additionally, we assume that the two samples have unequal variances since it is unlikely that the duration of TV shows added in 2020 and 2021 would have the exact same variance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "**Alternative Hypothesis**: The proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "tv_proportion = np.sum(df_tvshows['Country'].str.contains('United States')) / len(df_tvshows)\n",
        "movie_proportion = np.sum(df_movies['Country'].str.contains('United States')) / len(df_movies)\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(tv_proportion * len(df_tvshows)), int(movie_proportion * len(df_movies))]\n",
        "nobs = [len(df_tvshows), len(df_movies)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We conclude that the proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "z8imAO69Fp_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used to obtain P-Value is a two-sample proportion test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose this specific statistical test because it is appropriate for comparing two proportions, and it helps us to determine whether the difference between the two proportions is due to chance or not."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the continous value feature in a separate list\n",
        "continous_value_feature= [\"release_year\",\"duration\",\"day_added\",\"month_added\",\"year_added\"]\n",
        "\n",
        "# checking outliers with the help of box plot for continous features\n",
        "plt.figure(figsize=(16,5))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(1, 5, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing"
      ],
      "metadata": {
        "id": "0uzoriTJGLEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are taking the copied dataframe as the data having more number of observations resulted in ram exhaustion.\n",
        "df.shape, df_new.shape\n",
        "\n",
        "((175807, 14), (7770, 12))\n",
        "\n",
        "# Binning of rating in new dataframe\n",
        "df_new['rating'].replace(rating_map, inplace = True)\n",
        "\n",
        "# Checking sample after binning\n",
        "df_new.sample(2)"
      ],
      "metadata": {
        "id": "fRlBNRTRGT57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Textual Columns"
      ],
      "metadata": {
        "id": "RChmXzhkGYco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new feature content_detail with the help of other textual attributes\n",
        "df_new[\"content_detail\"]= df_new[\"cast\"]+\" \"+df_new[\"director\"]+\" \"+df_new[\"listed_in\"]+\" \"+df_new[\"type\"]+\" \"+df_new[\"rating\"]+\" \"+df_new[\"country\"]+\" \"+df_new[\"description\"]\n",
        "\n",
        "#checking the manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "yBqz-4-CGecf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Lower Casing"
      ],
      "metadata": {
        "id": "9xMQM0JlGiEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df_new['content_detail']= df_new['content_detail'].str.lower()\n",
        "\n",
        "# Checking the manipulation\n",
        "df_new.iloc[281,]['content_detail']\n",
        ""
      ],
      "metadata": {
        "id": "LhjKHfUzGlAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Removing Punctuations"
      ],
      "metadata": {
        "id": "HUek_YC8Go4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove punctuations\n",
        "def remove_punctuations(text):\n",
        "    '''This function is used to remove the punctuations from the given sentence'''\n",
        "    #imorting needed library\n",
        "    import string\n",
        "    # replacing the punctuations with no space, which in effect deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped off punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "lUOUMMHdGsTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations from the content_detail\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_punctuations)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']\n",
        ""
      ],
      "metadata": {
        "id": "5FBlWp3yGvjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 Removing URLs & Removing words and digits contain digits.\n"
      ],
      "metadata": {
        "id": "mxvXPOW3GwmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and Numbers from the given sentence'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # Replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text= re.sub(url_number_pattern,'', text)\n",
        "\n",
        "    # Replacing the digits with one space\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # return the text stripped off URL's and Numbers\n",
        "    return text"
      ],
      "metadata": {
        "id": "BbM5IV23Gzcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_url_and_numbers)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']\n",
        "\n"
      ],
      "metadata": {
        "id": "Nxzvm57eG11I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.5 Removing Stopwords & Removing White spaces\n"
      ],
      "metadata": {
        "id": "KhetSVNRG3pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "id": "ETdz5wrHG-Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# create a set of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# displaying stopwords\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "vuCCIjOGG6K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_and_whitespaces(text):\n",
        "    '''This function is used for removing the stopwords from the given sentence'''\n",
        "    text = [word for word in text.split() if not word in stopwords.words('english')]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # removing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text\n",
        ""
      ],
      "metadata": {
        "id": "em6wNQN8HdPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "df_new['content_detail']= df_new['content_detail'].apply(remove_stopwords_and_whitespaces)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "nxLK1QCdHeua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['content_detail'][0]"
      ],
      "metadata": {
        "id": "KwJf7D-7Hlai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.6 Tokenization"
      ],
      "metadata": {
        "id": "ALX8DxL_HntX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "df_new['content_detail']= df_new['content_detail'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "wT6D9SV6HtKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.7 Text Normalization"
      ],
      "metadata": {
        "id": "u4cPNkC7H0X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Importing WordNetLemmatizer from nltk module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "kzXOX9YwH3li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizing_sentence(text):\n",
        "    '''This function is used for lemmatizing (changing the given word into meaningfull word) the words from the given sentence'''\n",
        "    text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text"
      ],
      "metadata": {
        "id": "CMxhKsl_Hm5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Rephrasing text by applying defined lemmatizing function\n",
        "df_new['content_detail']= df_new['content_detail'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.iloc[281,]['content_detail']"
      ],
      "metadata": {
        "id": "NiiEYLlaH_8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.8 Part of Speech tagging"
      ],
      "metadata": {
        "id": "Hr3pIfsVICr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text into words before POS Taging\n",
        "df_new['pos_tags'] = df_new['content_detail'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df_new.head(5)"
      ],
      "metadata": {
        "id": "enTzvy5qIF7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.9 Text Vectorization"
      ],
      "metadata": {
        "id": "X5fr-ZCfIJHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Importing needed libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=30000)"
      ],
      "metadata": {
        "id": "5o-sA5uPINI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting on TfidfVectorizer\n",
        "x= tfidfv.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Checking shape of the formed document matrix\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "7bOwLuFWIQnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 Dimesionality Reduction"
      ],
      "metadata": {
        "id": "QikcMekkITeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "# Importing PCA from sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Defining PCA object with desired number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fitting the PCA model\n",
        "pca.fit(x.toarray())\n",
        "\n",
        "# percent of variance captured by each component\n",
        "variance = pca.explained_variance_ratio_\n",
        "print(f\"Explained variance: {variance}\")"
      ],
      "metadata": {
        "id": "oxQSl4WYIW2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting the percent of variance captured versus the number of components in order to determine the reduced dimensions\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(variance)+1), np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Number of Components')\n",
        "ax.set_ylabel('Percent of Variance Captured')\n",
        "ax.set_title('PCA Analysis')\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5o2r-XiAIZl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It is clear from the above plot that 7770 principal components can capture the 100% of variance. For our case we will consider only those number of PC's that can capture 95% of variance"
      ],
      "metadata": {
        "id": "INnUtd96JzD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "pIQEJII1J0SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model -1 (K-Means Clustering)"
      ],
      "metadata": {
        "id": "OonK5KPNImEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "model = KMeans(random_state=0)\n",
        "visualizer = KElbowVisualizer(model, k=(1,16),locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "nN_cN_cfIqE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it seems that the elbow is forming at the 2 clusters but before blindly believing it let's plot one more chart that itterates over the same number of cluters and determines the Silhouette Score at every point.\n",
        "\n",
        "Okay, but what is Silhouette Score?\n",
        "\n",
        "The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. It is used to evaluate the quality of clustering, where a higher score indicates that objects are more similar to their own cluster and dissimilar to other clusters.\n",
        "\n",
        "The silhouette score ranges from -1 to 1, where a score of 1 indicates that the object is well-matched to its own cluster, and poorly-matched to neighboring clusters. Conversely, a score of -1 indicates that the object is poorly-matched to its own cluster, and well-matched to neighboring clusters."
      ],
      "metadata": {
        "id": "ekgoci8iI0rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer(model, k=(2,16), metric='silhouette', timings=True, locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x_transformed)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "WIbMe-W9I1c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Defining Range\n",
        "k_range = range(2, 7)\n",
        "for k in k_range:\n",
        "    Kmodel = KMeans(n_clusters=k)\n",
        "    labels = Kmodel.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "3j9BGRspI5Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots (Elbow plot and Sillhouette plot) it is very clear that the Silhoutte score is comparatively good for 4 number of clusters, so we will consider 4 cluster in kmeans analysis.\n",
        "\n",
        "Now let's plot and see how our data points look like after assigning to their respective clusters."
      ],
      "metadata": {
        "id": "kAC4F62GI7OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', random_state= 0)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = kmeans.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "b_f2ozi9I9DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 4 different clusters but unfortunately the above plot is in TWO-DIMENSIONAL. Let's plot the above figure in 3D using mplot3d library and see if we are getting the separated clusters."
      ],
      "metadata": {
        "id": "FyDHYNZMI-yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[kmeans.labels_ == i, 2], x_transformed[kmeans.labels_ == i, 0], x_transformed[kmeans.labels_ == i, 1], c=colors[i])\n",
        "\n",
        "# Rotate the plot 30 degrees around the X axis and 45 degrees around the Z axis\n",
        "ax.view_init(elev=20, azim=-120)\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucdpdxx5I_Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can easily differentiate the all 4 clusters with naked eye. Now let's assign the 'Conent' in their respective cluster by appending 1 more attribute in the final dataframe."
      ],
      "metadata": {
        "id": "mnXZ5O9GJClw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df_new['kmeans_cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "9K5no_vsJDxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Explain the ML Model used and it's performance ?**"
      ],
      "metadata": {
        "id": "Yz5u93wnJF0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    #Importing libraries\n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    df_wordcloud = df_new[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "WRgDD-oCJIxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
        "for i in range(4):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gT4dC4zaJL4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model -2 (Hierarchial Clustering)"
      ],
      "metadata": {
        "id": "Y7PysaJHK9nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing needed libraries\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# HIERARCHICAL CLUSTERING\n",
        "distances_linkage = linkage(x_transformed, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(25, 10))\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('All films/TV shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distances_linkage, no_labels = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y75gIuOkLBa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. what is Dendogram and how to determine the optimal value of clusters?\n",
        "\n",
        "* A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.\n",
        "* From the above Dendogram we can say that optimal value of clusters is 2. But before assigning the vlaues to respective clusters, let's check the silhouette scores using Agglomerative clustering and follow the bottom up approach to aggregate the datapoints."
      ],
      "metadata": {
        "id": "J4BYXqKiLH_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "XAQG7v0uLOBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above silhouette scores it is clear that the 2 clusters are optimal value (maximum Silhouette score), which is also clear from the above Dendogram that for 2 cluters the euclidean distances are maximum.\n",
        "\n",
        "Let's again plot the chart and observe the 2 different formed clusters."
      ],
      "metadata": {
        "id": "XjwJDJN4LQE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "7ZPaFh4TLRbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2],c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LnveW0OTLUC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can again easily differentiate the all 2 clusters with naked eye. Now let's assign the 'Content(Movies and TV Shows)' in their respective cluster by appending 1 more attribute in the final dataframe."
      ],
      "metadata": {
        "id": "q_uRaTlMLV8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df_new['agglomerative_cluster'] = Agmodel.labels_"
      ],
      "metadata": {
        "id": "0HfOgtaHmyGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "bC3EJ1Q9LXpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just again define a function that plots wordcloud for different attributes using Agglomerative Clustering."
      ],
      "metadata": {
        "id": "YyeM6GihLZt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  #Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df_new[['agglomerative_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "qmctsEw6LYlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N6Pj3th2mmDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 (Building a Recommendaton System)"
      ],
      "metadata": {
        "id": "sg8HrFnILeP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Cosine similarity as it is a measure of similarity between two non-zero vectors in a multidimensional space. It measures the cosine of the angle between the two vectors, which ranges from -1 (opposite direction) to 1 (same direction), with 0 indicating orthogonality (the vectors are perpendicular to each other).\n",
        "\n",
        "In this project we have used cosine similarity which is used to determine how similar two documents or pieces of text are. We represent the documents as vectors in a high-dimensional space, where each dimension represents a word or term in the corpus. We can then calculate the cosine similarity between the vectors to determine how similar the documents are based on their word usage.\n",
        "\n",
        "\n",
        "We are using cosine similarity over tf-idf because:\n",
        "\n",
        "\n",
        "* Cosine similarity handles high dimensional sparse data better.\n",
        "\n",
        "* Cosine similarity captures the meaning of the text better than tf-idf. For example, if two items contain similar words but in different orders, cosine similarity would still consider them similar, while tf-idf may not. This is because tf-idf only considers the frequency of words in a document and not their order or meaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "SZ2gl_MDLg_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing neede libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a TF-IDF vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df_new['content_detail'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=df_new):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "    return rec_table\n",
        ""
      ],
      "metadata": {
        "id": "txTbcv4vLe-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian movie\n",
        "recommend_content('Dil Chahta Hai')"
      ],
      "metadata": {
        "id": "5uJqb9icLtkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian movie\n",
        "recommend_content('The Edge of Seventeen')"
      ],
      "metadata": {
        "id": "WVW6IzzyL4Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian tv show\n",
        "recommend_content('Humsafar')"
      ],
      "metadata": {
        "id": "fXtmTrUkL6Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian tv show\n",
        "recommend_content('Dracula')\n",
        ""
      ],
      "metadata": {
        "id": "QljaTPoUL8FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "dMm0WWSlTWpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have chosen Silhoutte Score over Distortion Score (also known as inertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other clusters. It ranges from -1 to 1, with higher values indicating better cluster separation. A silhouette score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters. A score close to 0 indicates that the data point is on or very close to the boundary between two clusters. A score close to -1 indicates that the data point is probably assigned to the wrong cluster.\n",
        "\n",
        "\n",
        "The advantages of using silhouette score over distortion score are:\n",
        "\n",
        "\n",
        "* Silhouette score takes into account both the cohesion (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only considers the compactness of each cluster.\n",
        "* Silhouette score is less sensitive to the shape of the clusters, while distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "* Silhouette score provides more intuitive and interpretable results, as it assigns a score to each data point rather than just a single value for the entire clustering solution."
      ],
      "metadata": {
        "id": "3yn4WV2vTZ6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "unxFYp3TTh0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have considered K-means as our final model, as we are getting the comparatevely high Silhoutte Score in K-means clustering and the resulted clusters are very well seperated from each others as have saw in the 3 dimensions.\n",
        "\n",
        "\n",
        "Also in some of the situations K-means works more accurately then other clustering methods such as:\n",
        "\n",
        "\n",
        "\n",
        "**Speed**: K-means is generally faster than hierarchical clustering, especially when dealing with large datasets, since it involves fewer calculations and iterations.\n",
        "\n",
        "**Ease of use**: K-means is relatively straightforward to implement and interpret, as it requires only a few parameters (such as the number of clusters) and produces a clear partitioning of the data.\n",
        "\n",
        "**Scalability**: K-means can easily handle datasets with a large number of variables or dimensions, whereas hierarchical clustering becomes computationally expensive as the number of data points and dimensions increase.\n",
        "\n",
        "**Independence of clusters**: K-means produces non-overlapping clusters, whereas hierarchical clustering can produce overlapping clusters or clusters that are nested within each other, which may not be ideal for certain applications."
      ],
      "metadata": {
        "id": "GeaE_KO5TioJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions drawn from EDA**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the exploratory data analysis (EDA) of the Netflix movies and TV shows clustering dataset, we have drawn the following conclusions:**"
      ],
      "metadata": {
        "id": "hatKl8c2T1_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Movies make up about two-thirds of Netflix content, with TV shows comprising the remaining one-third.\n",
        "\n",
        "* Adult and teen categories are prevalent on Netflix, while family-friendly content is more common in TV shows than in movies.\n",
        "\n",
        "* Indian actors dominate Netflix movies, while popular Indian actors are absent from TV shows.\n",
        "\n",
        "* Jan Suter is the most common movie director, and Ken Burns is the most common TV show director on Netflix.\n",
        "\n",
        "* The United States is the largest producer of movies and TV shows on Netflix, followed by India. Japan and South Korea have more TV shows than movies, indicating growth potential in that area.\n",
        "\n",
        "* International movies, drama, and comedy are the most popular genres on Netflix.\n",
        "\n",
        "* TV show additions on Netflix have increased since 2018, while movie additions have decreased. In 2020, fewer movies were added compared to 2019, but more TV shows were added.\n",
        "\n",
        "* October, November, and December are popular months for adding TV shows, while January, October, and November are popular for adding movies. February sees the least additions.\n",
        "\n",
        "* Movies and TV shows are typically added at the beginning or middle of the month and are popularly added on weekends.\n",
        "\n",
        "* Most movies on Netflix have durations between 80 to 120 minutes, while TV shows commonly have one or two seasons.\n",
        "\n",
        "* Various countries contribute adult and teen content, with Spain producing the most adult content and Canada focusing on children and family-friendly categories."
      ],
      "metadata": {
        "id": "y1RRB6q4T3TK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions drawn from ML Model"
      ],
      "metadata": {
        "id": "TE9yha1IULka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implemented K-Means Clustering and Agglomerative Hierarchical Clustering, to cluster the Netflix Movies TV show dataset.\n",
        "\n",
        "* The optimal number of clusters we are getting from K-means is 4, whereas for Agglomerative Hierarchical Clustering the optimal number of clusters are found out to be 2.\n",
        "* We chose Silhouette Score as the evaluation metric over distortion score because it provides a more intuitive and interpretable result. Also Silhouette score is less sensitive to the shape of the clusters.\n",
        "* Built a Recommendation system that can help Netflix improve user experience and reduce subscriber churn by providing personalized recommendations to users based on their similarity scores."
      ],
      "metadata": {
        "id": "AK51qHYtUPzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}